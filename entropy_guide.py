#!/usr/bin/env python3
"""
Entropy Optimization Testing Script

This script helps you test different entropy configurations for the residual model
and provides real-time monitoring of entropy reduction.
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.config.training_config import ConfigFactory

def show_entropy_comparison():
    """Show entropy settings for different configurations"""
    print("üéØ ENTROPY CONFIGURATION COMPARISON")
    print("=" * 60)
    
    configs = {
        'residual_model': 'Residual Model (Decisive)',
        'residual_low_entropy': 'Residual Model (Ultra Decisive)',
        'value_regularized_multi_epoch': 'Original Multi-Epoch'
    }
    
    for config_name, description in configs.items():
        try:
            config = ConfigFactory.create(config_name)
            
            print(f"\nüìã {description}")
            print(f"   Config: {config_name}")
            print(f"   üé≤ Initial entropy coeff: {config.ppo.entropy_coeff_start}")
            print(f"   üìâ Entropy decay: {config.ppo.entropy_coeff_decay}")
            print(f"   ‚è±Ô∏è  Decay episodes: {config.ppo.entropy_decay_episodes}")
            print(f"   üè† Placement entropy: {config.ppo.placement_entropy_coeff}")
            print(f"   üîó Edge entropy: {config.ppo.edge_entropy_coeff}")
            print(f"   ‚öîÔ∏è  Army entropy: {config.ppo.army_entropy_coeff}")
            
            # Calculate entropy factor at different episodes
            episodes_to_test = [0, 100, 500, 1000, 2000, 5000]
            print(f"   üìä Entropy factor over time:")
            for ep in episodes_to_test:
                factor = max(config.ppo.entropy_coeff_start - (ep / config.ppo.entropy_decay_episodes) * config.ppo.entropy_coeff_decay, 0.01)
                print(f"      Episode {ep:4d}: {factor:.3f}")
                
        except Exception as e:
            print(f"   ‚ùå Error loading {config_name}: {e}")

def show_recommendations():
    """Show specific recommendations for entropy tuning"""
    print("\nüí° ENTROPY TUNING RECOMMENDATIONS")
    print("=" * 60)
    
    print("""
üîç DIAGNOSIS: High Entropy = Random Actions
   Your residual model has stable gradients but high entropy means:
   ‚Ä¢ Actions are too random/uniform
   ‚Ä¢ Not learning decisive strategies
   ‚Ä¢ Need stronger entropy reduction to encourage focused actions

üéØ IMMEDIATE SOLUTIONS:

   1Ô∏è‚É£ MODERATE SOLUTION (Try First):
      Change line 55 in RLGNNAgent.py:
      config = ConfigFactory.create('residual_model')  # Current
      # This uses aggressive but reasonable entropy reduction

   2Ô∏è‚É£ AGGRESSIVE SOLUTION (If still too random):
      config = ConfigFactory.create('residual_low_entropy')
      # This uses very aggressive entropy reduction

üîß WHAT THESE CHANGES DO:

   RESIDUAL_MODEL (Moderate):
   ‚Ä¢ Starts with entropy factor 1.0 (vs 0.5 before)
   ‚Ä¢ Decays 90% over 5000 episodes (vs 60% over 15000)
   ‚Ä¢ Higher entropy weights for placement and edge selection
   ‚Ä¢ Should see entropy drop within 500-1000 episodes

   RESIDUAL_LOW_ENTROPY (Aggressive):
   ‚Ä¢ Starts with entropy factor 2.0 (very high initial exploration)
   ‚Ä¢ Decays 90% over just 2000 episodes (very fast)
   ‚Ä¢ Much higher entropy weights
   ‚Ä¢ Should see rapid entropy drop within 200-500 episodes

üìä MONITORING EXPECTATIONS:

   Episode 100:  Army entropy should drop from ~1970 to ~1500
   Episode 500:  Army entropy should be ~800-1200
   Episode 1000: Army entropy should be ~400-800
   Episode 2000: Army entropy should be ~200-400 (decisive actions)

‚ö†Ô∏è  IMPORTANT NOTES:
   ‚Ä¢ These configs maintain gradient stability of residual model
   ‚Ä¢ Higher entropy coefficients = stronger pressure toward decisive actions
   ‚Ä¢ Monitor both entropy AND performance (win rate, regions captured)
   ‚Ä¢ If entropy drops too fast, actions might become deterministic too early

üéÆ GAME BEHAVIOR CHANGES EXPECTED:
   ‚Ä¢ More consistent army placement strategies
   ‚Ä¢ Less random attack selection
   ‚Ä¢ More focused offensive/defensive patterns
   ‚Ä¢ Better continent control attempts
    """)

def show_quick_commands():
    """Show quick commands for testing"""
    print("\n‚ö° QUICK TEST COMMANDS")
    print("=" * 60)
    
    print("""
üîß TO APPLY MODERATE ENTROPY REDUCTION:
   Edit line 55 in src/agents/RLGNNAgent.py:
   config = ConfigFactory.create('residual_model')

üîß TO APPLY AGGRESSIVE ENTROPY REDUCTION:
   Edit line 55 in src/agents/RLGNNAgent.py:  
   config = ConfigFactory.create('residual_low_entropy')

üìä TO MONITOR ENTROPY IN TENSORBOARD:
   1. Run training for 500+ episodes
   2. Open TensorBoard: tensorboard --logdir analysis/logs
   3. Watch these metrics:
      ‚Ä¢ army_entropy_mean (should decrease over time)
      ‚Ä¢ placement_entropy_mean (should decrease over time)
      ‚Ä¢ edge_entropy_mean (should decrease over time)

üéØ SUCCESS INDICATORS:
   ‚úÖ Army entropy decreasing from ~1970 to <500 within 1000 episodes
   ‚úÖ More consistent placement patterns
   ‚úÖ Gradient norms staying stable (10-100 range)
   ‚úÖ Win rate improving or stable while entropy decreases

‚ùå FAILURE INDICATORS:
   ‚ùå Entropy stays high (>1500) after 1000 episodes
   ‚ùå Gradient norms exploding (>1000)
   ‚ùå Performance degrading significantly
   ‚ùå Actions becoming deterministic too early (entropy <50)
    """)

def main():
    """Main function"""
    print("üé≤ ENTROPY OPTIMIZATION GUIDE FOR RESIDUAL MODEL")
    print("=" * 70)
    
    show_entropy_comparison()
    show_recommendations() 
    show_quick_commands()
    
    print("\n‚úÖ Ready to optimize entropy! Start with 'residual_model' config.")

if __name__ == "__main__":
    main()
